---
title: "Common Social Sciences variables"
author: "Aaron Rudkin"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Common Social Sciences variables}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo=FALSE}
options(digits=2)
set.seed(19861108)
library(fabricatr)
```

**fabricatr** makes it easy to generate common social science variables. While our other online tutorials focus on exploring the full extent of **fabricatr**'s capabilities, this tutorial focuses on simple plain-English introductions to common variable types used in social science research, along with an introduction to the statistical terminology associated with them. Examples are drawn from pre-analysis plans filed with [http://egap.org/](Evidence in Governance and Politics) (EGAP). 

**If you already feel comfortable understanding these concepts and would prefer a technical manual for our variable creation functions, please consult our [Generating Variables](http://fabricatr.declaredesign.org/articles/variable_generation.html) tutorial.**

## Binary outcomes (e.g. turnout)

The simplest possible outcome of interest is a binary outcome: one that can be either true or false. An example of this sort of outcome in a social science research context is whether or not a subject performs an action: Does a person seek medical treatment? Does a person turn out to vote? Does a person write a letter to their congressional representative? Is an infrastructure project completed on time? These sorts of questions are typically modelled as binary outcomes. The "yes" or "no" answer to your question of interest is represented as 1 ("yes") or 0 ("no") numerically.

In **fabricatr**, binary data can easily be modeled with the `draw_binary()` function:

```{r echo=TRUE, results="hide"}
voter_turnout = draw_binary(prob = 0.4, N = 100)

table(voter_turnout)
```
```{r echo=FALSE}
knitr::kable(table(voter_turnout))
```

`draw_binary` requires that you supply two pieces of data: `prob`, which specifies either a single probability or a vector of probabilities (one for each unit), the probability of getting a "1" in your simulated data. For instance, here we are modeling a turnout rate of 40%. Second, it requires `N`, which specifies how many observations to create.

It is also possible to specify the probability stochastically. Imagine if you wish to model a population where individuals under age 40 have a low turnout rate, while individuals above age 40 have a high turnout rate. This is very simple with **fabricatr**:

```{r}
population <- fabricate(
  N = 100,
  age = round(runif(N, 18, 85)),
  turnout = draw_binary(prob = ifelse(age < 40, 0.4, 0.7), N=N)
)
```

This example introduces you to a few new functions. First, we can wrap multiple variable creation commands in a `fabricate` call. This will ensure the result is a data frame, and that each variable creation command has access to the previous variables. Second, you see that we can model age using `R`'s built-in `runif` function. If you would like more information about modeling statistical distributions in R, see the `?Distributions` manual page. Third, you see that we model turnout probability statistically; if the user has an age below 40, they receive turnout probability 0.4. If not, they receive turnout probability 0.7.

As noted in the introduction for this tutorial, the target audience for this tutorial is someone interested in common social science outcome variables, but without a strong background in thinking about modelling these variables. If you feel comfortable modelling but need to learn how to specify the variables in **fabricatr**, please see our tutorial [on variable generation with **fabricatr**](http://fabricatr.declaredesign.org/articles/variable_generation.html). If you need more information about modeling the structure of data or implementing multi-level models, panel data, or cross-classified data with **fabricatr**, then please see our tutorial [on building and importing data](http://fabricatr.declaredesign.org/articles/building_importing.html).

### Ordered data (e.g. satisfaction)

The type of data generated for many survey questions is "ordered". Ordered data includes the "Likert scale", and is used when respondents to a survey report an outcome which has a logical ordering from lowest to highest but is not necessarily measured in numerically comparable terms. Consider a basic question like "Do you approve of your mayor's job performance?". There are many ways you could measure a respondent's answer to this question, but one of the most common is by offering the respondent the choice to "Strongly Disapprove", "Disapprove", feel "Neutral", "Approve", or "Strongly Approve". This five point scale is ordered data.

When simulating ordered data, we typically think of the problem differently, as a two-step process. The first step is to assign a respondent a score over some range, and the second step is to translate that score into the categories of the ordered variable. We call the first step a "latent variable". In the real survey, we never see the latent variable, only the ordered data outcome. But we can rely on the latent variable to help shape our simulated data.

We generate a simulated latent variable and then translate it into a simulated ordered variable, like so:

```{r echo=FALSE, fig.width=7, fig.height=3}
latent_example = function(x, density_x) {
  current_margins = par("mar")
  density_x = 0.4 * (density_x / max(density_x))
  par(mar=c(1,1,1,1))
  plot(x,
       density_x,
       main="",
       xaxt="n",
       yaxt="n",
       type="l",
       bty="n",
       xlim=c(-3, 3),
       ylim=c(-0.2, 0.5)
  )
  text(0, -0.1, "Latent Variable")
  
  for(x in c(-1.5, -0.5, 0.5, 1.5)) {
    lines(c(x, x), c(-0.03, 0.5), lty=2, col="lightgrey")
  }
  text(-2, 0.45, "Strongly Disagree")
  text(-1, 0.45, "Disagree")
  text(0, 0.45, "Neutral")
  text(1, 0.45, "Agree")
  text(2, 0.45, "Strongly Agree")

  lines(c(-2.8, 2.8), c(-0.03, -0.03))
  lines(c(-2.7, -2.8, -2.7), c(0, -0.03, -0.06))
  lines(c(2.7, 2.8, 2.7), c(0, -0.03, -0.06))
  par(mar=current_margins)
}

latent_example(x = seq(-2.8, 2.8, 0.001), density_x = dnorm(seq(-2.8, 2.8, 0.001)))
```

In the above graph, most respondents (the "density" represented by the height of the curve) are neutral about the mayor, and relatively few hold extreme opinions. We generated a "normal" latent variable for our respondents and then translate this to the ordered outcome. The simplest normal variable is the "standard normal", a random variable whose mean (center) is 0, and whose standard deviation (spread) is 1. So, although it is not shown in the graph above, the dashed lines which divide the ordered outcomes are located at -1.5, -0.5, 0.5, and 1.5. 

Of course it would also be possible to affect the center and spread of this question:

```{r echo=FALSE, fig.width=7, fig.height=3}
latent_example(x = seq(-2.8, 2.8, 0.001), density_x = dnorm(seq(-3.8, 1.8, 0.001), sd=0.5))
```

Although thinking about ordered data in this latent context may seem strange at first, actually generating an ordered data outcome in **fabricatr** is quite easy:

```{r}
mayor_approval <- draw_ordered(x = rnorm(n = 100),
                               breaks = c(-1.5, -0.5, 0.5, 1.5),
                               break_labels = c("Strongly Disagree", "Disagree",
                                                "Neutral", "Agree",
                                                "Strongly Agree"))

table(mayor_approval)
```

Let's look at this `draw_ordered` example. `draw_ordered` requires three pieces of information from us: `x` (the latent variable), `breaks` (the places in the latent variable which divide the categories in the ordered outcome), and `break_labels` (which label the resulting ordered outcome). 

For `x`, we use the `rnorm` command to generate 100 standard normal draws representing our respondents. For `breaks` and `break_labels`, we mark and label the breaks as we did above.

The appeal of **fabricatr** is that we can create relationships between variables, so let's imagine a slightly more complex data where in general the mayor is well liked by members of her political party, but not by members of the opposite political party. To do this, we'll need to assign respondents to a political party (we'll mark the mayor's political party with "1" and the opposition political party with "0") and then generate our latent variable.

First, let's visualize the latent variable:

```{r echo=FALSE, fig.width=7, fig.height=3}
latent_example_parties = function(x, density_x, density_x2) {
  current_margins = par("mar")
  max_density = max(max(density_x), max(density_x2))
  density_x = 0.4 * (density_x / max_density)
  density_x2 = 0.4 * (density_x2 / max_density)
  par(mar=c(1,1,1,1))
  plot(x,
       density_x,
       col="red",
       main="",
       xaxt="n",
       yaxt="n",
       type="l",
       bty="n",
       xlim=c(-3, 3),
       ylim=c(-0.2, 0.5)
  )
  lines(x, density_x2, col="blue")
  text(0, -0.1, "Latent Variable")
  
  for(x in c(-1.5, -0.5, 0.5, 1.5)) {
    lines(c(x, x), c(-0.03, 0.5), lty=2, col="lightgrey")
  }
  text(-2, 0.45, "Strongly Disagree")
  text(-1, 0.45, "Disagree")
  text(0, 0.45, "Neutral")
  text(1, 0.45, "Agree")
  text(2, 0.45, "Strongly Agree")

  lines(c(-2.8, 2.8), c(-0.03, -0.03))
  lines(c(-2.7, -2.8, -2.7), c(0, -0.03, -0.06))
  lines(c(2.7, 2.8, 2.7), c(0, -0.03, -0.06))
  par(mar=current_margins)
}

latent_example_parties(x = seq(-2.8, 2.8, 0.001), 
               density_x = dnorm(seq(-3.8, 1.8, 0.001), sd=0.7),
               density_x2 = dnorm(seq(-1.8, 3.8, 0.001), sd=0.7)
               )
```

Here, colors indicate the two parties. We see that some members of the red party do disagree with the mayor, but the overwhelming majority do not, and vice versa for the blue party. Now, let's put our visualization into action:

```{r}
respondent_data <- fabricate(
  N = 100,
  mayor_copartisan = draw_binary(prob = 0.6, N),
  mayor_approval = draw_ordered(
    x = rnorm(N, mean = -1 + 2 * mayor_copartisan),
    breaks = c(-1.5, -0.5, 0.5, 1.5),
    break_labels = c("Strongly Disagree", "Disagree", "Neutral",
                     "Agree", "Strongly Agree")
  )
)

table(respondent_data$mayor_approval, respondent_data$mayor_copartisan)
```

This example is a little more complex. You can see the use of the `fabricate` wrapper function, which is described in more detail in our [Building and Importing Data](http://fabricatr.declaredesign.org/articles/building_importing.html) guide. Additionally, the latent variable `x` is more complex. Let's consider the formula for how this variable is specified. We know that the latent variable space from -1.5 to -0.5 indicates "Disagree". For respondents who have `mayor_copartisan` equal to 0 (they are not in the mayor's political party), their preference will be a standard normal draw centered at $-1 + (2 * 0) = -1$. For respondents who have `mayor_copartisan` equal to 1 (they are in the mayor's political party), their preference will be a standard normal draw centered at $-1 + (2 * 1) = 1$ -- "Agree".

It is easy to play with the location of breaks, labels for breaks, and format of the underlying latent variable. Here's an example where the latent variable is "uniform" -- there is an equal likelihood of choosing each of the options:

```{r}
mayor_approval <- draw_ordered(x = runif(n = 100, min = -2.5, max = 2.5),
                               breaks = c(-1.5, -0.5, 0.5, 1.5),
                               break_labels = c("Strongly Disagree", "Disagree",
                                                "Neutral", "Agree",
                                                "Strongly Agree"))
```

Users who are new to the `R` programmming language and want to know more about other statistical distributions beyond the `norm` and `unif` distributions used here, should run the command `?Distributions` in the R console. 

### Likert Data

Many survey responses that focus on labeling agreement, support, or quality evaluation, including the example above, are called "Likert" data after the psychologist Rensis Likert. These scales are typically 4, 5, or 7 point scales of a measure. **fabricatr** includes a simple shortcut to make Likert variables without needing to fill out the `breaks` and `break_labels` each time.

By using `draw_likert`, users only need to specify the `x` latent variable -- assumed to be distributed with breaks spaced 1 unit apart and with the data centered on 0 -- and the `type` of Likert they would like (the number of categories). In this example, we examine the same data above, but with a 4-category Likert scale where the options are "Strongly Agree", "Agree", "Disagree", and "Strongly Disagree", with no neutral category.

```{r}
mayor_approval <- draw_likert(x = rnorm(n = 100),
                              type = 4)

table(mayor_approval)
```

### Categorical data (e.g. demographic measures)

Surveys often collect demographic information from respondents, including age, gender, and ethnicity. By this point in our guide, you have begun to see how you might generate a continuous numerical variable like age, or how you traditional binary measures of gender might be specified. But some variables, like inclusive measures of gender, or ethnicity, (or hair color, eye color, city of residence, or many other possible variables of interest) are not "ordered" in any sense.

These data are often called "categorical" data -- a given person has a probability to belong to each of the possible categories.

Imagine that researchers are conducting a survey in Kenya. They wish to capture the country's four largest ethnic groups: Kikuyu, Luhya, Kalenjin, Luo, and are also interested in capturing the smaller Maasai group. Other respondents are identified as "Other". When preparing their research, the researchers must simulate the ethnicity of their expected respondents, so they first gather the proportions of each group,

```{r echo=FALSE}
total_pop = 38610097 
group_counts = c(6622576, 5338666, 4967328, 4044440, 841622)
other_count = total_pop - sum(group_counts)
display_counts = c(group_counts, other_count)
names(display_counts) = c("Kikuyu", "Luhya", "Kalenjin", "Luo", "Maasai", "Other")
prop.table(display_counts)
```

**fabricatr** makes it easy to generate data based on specifications like these using `draw_categorical`:

```{r echo=FALSE}
respondent_ethnicity <- draw_categorical(
  prob = c(0.172, 0.138, 0.129, 0.105, 0.022, 0.435),
  category_labels = c("Kikuyu", "Luhya", "Kalenjin", "Luo", "Maasai", "Other"),
  N = 100)

table(respondent_ethnicity)
```
### Data with fixed minimum and maximum values

It is often useful to generate data with fixed minimum or maximum values, but unclear how to best do so. In general the approach we would advocate to doing this is to generate a latent variable (described above) which might fall outside the minimum and maximum values, and then truncating the variable to the minimum and maximum values.

Consider an experiment with a pre-post treatment design. A series of respondents are asked about their views on government efficacy, which will be measured by the respondents scoring their government from 0 to 100. Some respondents are then given a treatment intervention: for example, information about government service delivery, or a mechanism to report government fraud or waste. Finally, researchers ask the same respondents the same question about government efficacy.

Researchers might simulate this experiment by choosing a population level Average Treatment Effect (in our example, assume a moderate treatment effect of 15 points), and adding a shock centered at this point to each treated unit's pre-treatment outcome (in addition to adding noise to the control group). A challenge with this data is that it is naturally truncated: individuals whose scores were already quite high may now be modeled as having post-treatment scores above 100.

```{r results="hide"}
set.seed(19861108)

efficacy_experiment <- fabricate(
  N = 1000,
  treatment = draw_binary(0.5, N),
  pre_outcome = rnorm(N, mean = 70, sd = 15),
  post_outcome = pre_outcome + rnorm(N, 
                                     mean = ifelse(treatment, 15, 0), 
                                     sd = 10)
)
```


In fact, in this example, some respondents have pre-treatment scores above 100! This matters because the real-world data gathered later will be truncated, and so the researchers power to detect effects may be compromised because of a high density of responses near the cutoff. As a result, it is important that researchers model the truncation directly. Doing so is simple using functionality provided by `R`: `pmin` and `pmax`.

When used to truncate variables like this, `pmax` takes two arguments: a vector of numeric variables, and a minimum value you want to truncate to. Please note that, counter-intuitively, `pmax` is used to truncate to the **minimum** value and `pmin` is used to truncate to the **maximum** value. We see this design in action below:

```{r results="hide"}
set.seed(19861108)

fixed_efficacy_experiment <- fabricate(
  N = 1000,
  treatment = draw_binary(0.5, N),
  pre_outcome = pmin(
    pmax(rnorm(N, mean = 70, sd = 15), 0),
    100),
  post_outcome = pmin(
    pmax(pre_outcome + rnorm(N, 
                             mean = ifelse(treatment, 15, 0), 
                             sd = 10), 0),
    100)
)
```

Let's consider how `pre_outcome` is created: first, each respondent is assigned a pre_treatment score with a mean of 70 points and a standard deviation of 15 points. Next, each of those scores is fed to `pmax`, along with an argument 0. Any scores that are lower than 0 are bumped up and truncated to 0. Finally, these scores are fed to `pmin`, along with an argument 100. Any scores that are higher than 100 are bumped down and truncated to 100. The result is that our variables display the desired effect:

```{r echo=FALSE, fig.width=7, fig.height=3}
hist(fixed_efficacy_experiment$post_outcome,
     main = "Histogram of post-treatment outcomes",
     yaxt = "n",
     breaks=c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100),
     xlab = "Post-Treatment Outcome: Score from 0 to 100")
```

### Outcomes with Intra-Cluster Correlation


